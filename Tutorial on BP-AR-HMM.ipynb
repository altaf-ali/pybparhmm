{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A featural model for relating multiple time series\n",
    "\n",
    "In our application of interest, we are faced with a collection of N time series representing realizations of related dynamical phenomena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per series dynamics\n",
    "We model the dynamics of each time series as an autoregressive Hidden Markov Model (AR-HMM).<br>\n",
    "<br>\n",
    "<center> $z_t|z_{t-1} \\sim \\pi_{z_{t-1}}$ </center>\n",
    "<br>\n",
    "<center> $y_t = \\sum_{l=1}^{r} A_{l,z_t}y_{t-l} + e_t(z_t) = \\bf{A}_k \\hat{y}_t + e_t(z_t)$ </center>\n",
    "<br>\n",
    "where $e_t(z_t) \\sim \\mathcal{N}(0, \\Sigma_{z_t})$ and $\\hat{y}_t = L^r(y_t)$ are the aggregated past observations.  We refer to $\\bf{A}_k=[A_{1,k}, \\dots, A_{r,k}]$ as the set of lag matrices.  Throughout, we denote the VAR parameters for the kth state as $\\theta_k = \\{A_k, \\Sigma_k\\}$ and refer to each VAR process as a dynamic behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a previous set of mode-specific transition probabilities $\\pi^{n-1}$, the global transition distribution $\\beta^{(n-1})$, and dynamic parameters $\\bf{\\theta}^{(n-1)}$:\n",
    "\n",
    "1. Set $\\bf{\\pi} = \\bf{\\pi}^{n-1}$, $\\bf{\\beta} = \\bf{\\beta}^{n-1}$, and $\\bf{\\theta} = \\bf{\\theta}^{n-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def backwards_message_vec(likelihood,blockEnd,pi_z,pi_s):\n",
    "    #Allocate storage space\n",
    "    Kz=pi_z.shape[1]\n",
    "    Ks=pi_s.shape[1]\n",
    "    T=length(blockEnd)\n",
    "    \n",
    "    bwds_msg = np.ones(Kz,T)\n",
    "    partial_marg = zeros(Kz,T)\n",
    "    #Compute marginalized likelihoods for all times, integrating s_t\n",
    "    if Kz==1 and Ks==1:\n",
    "        marg_like =  squeeze(likelihood)\n",
    "    else:\n",
    "        marg_like = squeeze(np.sum(likelihood * pi_s[:,:,np.ones(1,1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "from numpy.random import random\n",
    "from numpy import newaxis as na\n",
    "import scipy.stats as stats\n",
    "import scipy.linalg\n",
    "\n",
    "### Sampling functions\n",
    "\n",
    "def sample_discrete(dist,size=[]):\n",
    "    assert (dist >=0).all()\n",
    "    cumvals = np.cumsum(dist)\n",
    "    return np.sum(random(size)[...,na] * cumvals[-1] > cumvals, axis=-1)\n",
    "\n",
    "def sample_niw(mu_0,lmbda_0,kappa_0,nu_0):\n",
    "    '''\n",
    "    Returns a sample from the normal/inverse-wishart distribution, conjugate\n",
    "    prior for (simultaneously) unknown mean and unknown covariance in a\n",
    "    Gaussian likelihood model. Returns covariance.  '''\n",
    "    # this is completely copied from Matlab's implementation, ignoring\n",
    "    # the copyright. I'm sorry.\n",
    "    # reference: p. 87 in Gelman's Bayesian Data Analysis\n",
    "\n",
    "    # first sample Sigma ~ IW(lmbda_0^-1,nu_0)\n",
    "    lmbda = sample_invwishart(lmbda_0,nu_0) # lmbda = np.linalg.inv(sample_wishart(np.linalg.inv(lmbda_0),nu_0))\n",
    "    # then sample mu | Lambda ~ N(mu_0, Lambda/kappa_0)\n",
    "    mu = np.random.multivariate_normal(mu_0,lmbda / kappa_0)\n",
    "\n",
    "    return mu, lmbda\n",
    "\n",
    "def sample_invwishart(lmbda,dof):\n",
    "    # TODO make a version that returns the cholesky\n",
    "    # TODO allow passing in chol/cholinv of matrix parameter lmbda\n",
    "    n = lmbda.shape[0]\n",
    "    chol = np.linalg.cholesky(lmbda)\n",
    "\n",
    "    if (dof <= 81+n) and (dof == np.round(dof)):\n",
    "        x = np.random.randn(dof,n)\n",
    "    else:\n",
    "        x = np.diag(np.sqrt(stats.chi2.rvs(dof-(np.arange(n)))))\n",
    "        x[np.triu_indices_from(x,1)] = np.random.randn(n*(n-1)/2)\n",
    "    R = np.linalg.qr(x,'r')\n",
    "    T = scipy.linalg.solve_triangular(R.T,chol.T).T\n",
    "    return np.dot(T,T.T)\n",
    "\n",
    "def sample_wishart(sigma, dof):\n",
    "    '''\n",
    "    Returns a sample from the Wishart distn, conjugate prior for precision matrices.\n",
    "    '''\n",
    "\n",
    "    n = sigma.shape[0]\n",
    "    chol = np.linalg.cholesky(sigma)\n",
    "\n",
    "    # use matlab's heuristic for choosing between the two different sampling schemes\n",
    "    if (dof <= 81+n) and (dof == round(dof)):\n",
    "        # direct\n",
    "        X = np.dot(chol,np.random.normal(size=(n,dof)))\n",
    "    else:\n",
    "        A = np.diag(np.sqrt(np.random.chisquare(dof - np.arange(0,n),size=n)))\n",
    "        A[np.tri(n,k=-1,dtype=bool)] = np.random.normal(size=(n*(n-1)/2.))\n",
    "        X = np.dot(chol,A)\n",
    "\n",
    "    return np.dot(X,X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "tmp={'a':1,'b':2}\n",
    "print(len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
